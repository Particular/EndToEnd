### Current action items

- [x] Restrict splunk to only accept from TC
- [ ] Document how to update and view results - @williambza
- [ ] Switch to async-first API @ramon
- [ ] Investigate batch sizing for `Task.WhenAll` @ramon
- [x] Implement persistence variables - @ramon
  - [x] Seed endpoints
  - [ ] How can we clean the tests up after running tests?
    - Maybe as part of the actual test?
     - [x] MSMQ
     - [x] RabbitMQ
     - [ ] SQL Transport
     - [ ] ASB
     - [x] ASQ
     - [ ] RavenDB
     - [ ] SQL Server (NHibernate)
     - [ ] Azure
    - Perhaps a script?
  - [x] Azure Storage Persistence V5 connection strings
- [ ] Create splunk dashboards - @igal + @hadi
 - [x] Add session and permutation ID to statistics info
 - [x] Collect required performance counters manually and report using the `Statistics` object
 - [ ] Is there a way to see a graph per test fixture? @william
 - [ ] Include memory usage graph
- [ ] Implement other test types (send, sendlocal, publish, sagas, distruter) - @dvdstalt
  - [x] Receive only
  - [x] Send only
  - [x] Sagas
    - [ ] Remove sagas from other tests
shared files project?)
- [ ] Collect results for multiple runs to see stability of tests
   - Feed results back to various transport/persistence owners
- [ ] Inform @andreas once we feel we are ready to start Stability tests
- [ ] Delete previous performance test code in ProductionTests repo

### Future actions

- [ ] Automate tests
- [ ] Notify when tests restuls are off by a percentage (a standard deviation?)
- [ ] Automate runs based on packages changed
- [ ] Can TC setup multiple VM's for a task?
 - Database VM
 - Transport VM
 - Runner VM
- [ ] Scale seeded messages based on pre-run warmup
- [ ] SQL Azure
- [ ] Multi-endpoints per process
 
 ### Completed actions

- [x] Remove hard dependency on splunk @tim
- [x] Tweak permutation string display to include label of on/off @william
- [x] Invert dependency chain between V5/V6, transports, persistences and tests project (investigate what _should_ actually be in the 
- [x] Run tests on environment - manually (perhaps with a PS script) - @williambza + @gbiellem
 - [x] Make sure session ID is shared between all test run instances
 - ~~[ ] Switch to PS script to run tests~~
 - [x] Setup environments: @gbiellem (1700USD per month if run 24 hours)
  - Queues
   - [x] SQL2012 (Queues), 
   - [x] Rabbit, 
   - [x] Azure Storage Queues
  - Persistence
   - [x] SQL2012 (Persistence),
   - [x] RavenDB v3, 
   - [x] Azure Storage Persistence
  - Runners:
   - [x] Master (MSMQ)
   - ~~[ ] Worker 3x (MSMQ)~~
 - Infrastructure
   - ~~[ ] DTC~~
- [x] Implement test source for test permutations of persistence and transport providers @hmemcpy 
- [X] Include Metrics.Net to track [required metrics](https://github.com/Particular/EndToEnd/blob/docs/docs/metrics.md) @williambza
- [x] Migrate performance test code
- [x] Decide on test variables
- [x] Decide on metrics to capture
- [x] Can we capture all metrics when running in an AppDomain? @hmemcpy
 - [x] Are RAM and CPU performance counters available in AppDomains?
  - We need to use a process as not all useful resource utiliization performance counters are available in the scope of an app domain.
 - [x] Is time to first message (TTFM) equal?
  - It seems TTFM for app domains is faster but we don't have a large collection of runs for comparison.
- [x] Test permutations will be generated by NUnit, but the actual stress tests will be run as processes in the final environment
- [x] Should the benchmark run different tests for each variable in isolation?
    - V1 - No
    - vNext - Yes (but also include a combined benchmark)
- [x] Define categories / test permutations
 - [x] Identify what is the generated code is currently testing? @dvdstelt 
   - [x] What are the shortcomings?
     - There is no clean up after the tests
   - [x] Investigate how to separate the seeding from the running of the tests
       - [x] Is a separate in-process appdomain feasible?
       - [x] Or should we create a separate exe to perform the seeding?
       - [x] What about cleanup? Need contextual information e.g. connection string
   - [x] Implement something similar to IProfile for endpoind start and stop for seeding and cleaning up etc.
   - [x] Can gcServer be configured in code? Confirm in test using NSB
     - Can not be set via code, will need a config transform
   - [x] x86 vs 64bit can be configured using corflags
   - [x] Look at build target instead of corflags?
   - [x] Is the capturing of metrics in-process going to affect the validity of the results?
     - We assume yes. We can extract the performance counter capturing to be out of process, but we will still have to capture the Message Counter metric in process.  All of the tests will be capturing similar metrics so the impact should be the same across all scenarios.
 - [x] Create NUnit fixtures for each category in [the google sheet](https://docs.google.com/spreadsheets/d/1avUW8Y5gpcPqTxIBaq7X5OXXaE4lDU0e0ZA9FDFNygs/edit#gid=771631393) where the fixture generates all permutations for all variable values in that category @ramonsmits 
    - [x] How would these isolated tests look?
    - [x] How are we going to generate tests for each isolated operation?
    - [x] Generate and configure endpoints from permutations
 - [x] Spike the output options for metrics @williambza
     - ~~[x] Investigate Elastic + Kibana - EOD 04/04/2016~~
     - ~~[x] Investigate Graphite - EOD 05/04/2016~~
     - ~~[x] Microsoft Power BI - EOD 04/04/2016~~
     - ~~[ ] Influx DB - EOD 05/04/2016~~
     - ~~[ ] Defined file structure for separate CSVs @ramonsmits~~
     - ~~[ ] Combined CSV + excel~~
     - [x] Splunk
        - [x] Replace Console.WriteLines with splunk trace output @williambza + @hadi
        - [x] Determine where we should deploy splunk
 - [x] Generating bin folders based on configurations and permutations @hmemcpy
 - [x] Setup tests for ASB and ASQ
