### Current action items

 - [x] Identify what is the generated code is currently testing? @dvdstelt 
   - [x] What are the shortcomings?
     - There is no clean up after the tests
   - [ ] Investigate how to separate the seeding from the running of the tests
       - [ ] Is a separate in-process appdomain feasible?
       - [ ] Or should we create a separate exe to perform the seeding?
       - [ ] What about cleanup?
   - [x] Is the capturing of metrics in-process going to affect the validity of the results?
     - We assume yes. We can extract the performance counter capturing to be out of process, but we will still have to capture the Message Counter metric in process.  All of the tests will be capturing similar metrics so the impact should be the same across all scenarios.
 - [x] Create NUnit fixtures for each category in [the google sheet](https://docs.google.com/spreadsheets/d/1avUW8Y5gpcPqTxIBaq7X5OXXaE4lDU0e0ZA9FDFNygs/edit#gid=771631393) where the fixture generates all permutations for all variable values in that category @ramonsmits 
    - [x] How would these isolated tests look?
    - [x] How are we going to generate tests for each isolated operation?
    - [ ] Generate and configure endpoints from permutations
 - [ ] Spike the output options for metrics @williambza
     - ~~~[x] Investigate Elastic + Kibana - EOD 04/04/2016~~~
     - [ ] Investigate Graphite - EOD 05/04/2016
     - ~~~[x] Microsoft Power BI - EOD 04/04/2016~~~
     - [ ] Influx DB - EOD 05/04/2016
     - ~[ ] Defined file structure for separate CSVs @ramonsmits~
     - ~[ ] Combined CSV + excel~
     - [x] Splunk @hadi
        - [ ] Investigate lightweight logger to replace Metrics.Net (splunk tracelog?)
        - [ ] Determine where we should deploy splunk
 - [ ] Include basic [test configurations](https://github.com/Particular/EndToEnd/blob/docs/docs/variables.md) @hmemcpy
 - [ ] Setup environments: @gbiellem (what is the hourly cost for the environment?)
   - Queues
    - [ ] SQL2012 (Queues), 
    - [ ] Rabbit, 
    - [ ] Azure Storage Queues
   - Persistence
     - [ ] SQL2012 (Persistence),
     - [ ] RavenDB v3, 
     - [ ] Azure Storage Persistence
   - Runners:
     - [ ] Master (MSMQ)
     - [ ] Worker 3x (MSMQ)
  - Infrastructure
     - [ ] DTC
 
 ### Future actions
 
- [ ] Collate metric results into usable format upon test completion
- [ ] Collect initial results and provide feedback for V6 Launch TF.
- [ ] Allow customization of maximum run-time (so we don't ever have unbounded runs)
- [ ] Implement test source for test permutations of persistence and transport providers @hmemcpy 
- [ ] Setup tests for ASB and ASQ
- [ ] Automate runs based on packages changed
- [ ] Can TC setup multiple VM's for a task?
 - Database VM
 - Transport VM
 - Runner VM
 
 ### Completed actions

- [X] Include Metrics.Net to track [required metrics](https://github.com/Particular/EndToEnd/blob/docs/docs/metrics.md) @williambza
- [x] Migrate performance test code
- [x] Decide on test variables
- [x] Decide on metrics to capture
- [x] Can we capture all metrics when running in an AppDomain? @hmemcpy
 - [x] Are RAM and CPU performance counters available in AppDomains?
  - We need to use a process as not all useful resource utiliization performance counters are available in the scope of an app domain.
 - [x] Is time to first message (TTFM) equal?
  - It seems TTFM for app domains is faster but we don't have a large collection of runs for comparison.
- [x] Test permutations will be generated by NUnit, but the actual stress tests will be run as processes in the final environment
- [x] Should the benchmark run different tests for each variable in isolation?
    - V1 - No
    - vNext - Yes (but also include a combined benchmark)
- [x] Define categories / test permutations
